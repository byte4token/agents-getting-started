{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation import IntentResolutionEvaluator, ResponseCompletenessEvaluator\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=\"2025-03-01-preview\",\n",
    "    azure_deployment=\"gpt4o\",\n",
    ")\n",
    " \n",
    "intent_resolution_evaluator = IntentResolutionEvaluator(model_config)\n",
    "response_completeness_evaluator = ResponseCompletenessEvaluator(model_config=model_config)\n",
    " \n",
    "# Evaluating query and response as strings\n",
    "# A positive example. Intent is identified and understood and the response correctly resolves user intent\n",
    "result = intent_resolution_evaluator(\n",
    "    query=\"What are the opening hours of the Eiffel Tower?\",\n",
    "    response=\"Opening hours of the Eiffel Tower are 9:00 AM to 11:00 PM.\",\n",
    ")\n",
    "print(result)\n",
    " \n",
    "# A negative example. Only half of the statements in the response were complete according to the ground truth  \n",
    "result = response_completeness_evaluator(\n",
    "    response=\"Itinery: Day 1 take a train to visit Disneyland outside of the city; Day 2 rests in hotel.\",\n",
    "    ground_truth=\"Itinery: Day 1 take a train to visit the downtown area for city sightseeing; Day 2 rests in hotel.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac63a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import FunctionTool, ToolSet\n",
    "from typing import Set, Callable, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define some custom python function\n",
    "def fetch_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the weather information for the specified location.\n",
    "\n",
    "    :param location (str): The location to fetch weather for.\n",
    "    :return: Weather information as a JSON string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # In a real-world scenario, you'd integrate with a weather API.\n",
    "    # Here, we'll mock the response.\n",
    "    mock_weather_data = {\"Seattle\": \"Sunny, 25°C\", \"London\": \"Cloudy, 18°C\", \"Tokyo\": \"Rainy, 22°C\"}\n",
    "    weather = mock_weather_data.get(location, \"Weather data not available for this location.\")\n",
    "    weather_json = json.dumps({\"weather\": weather})\n",
    "    return weather_json\n",
    "\n",
    "functions = FunctionTool({fetch_weather})\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)\n",
    "\n",
    "# Create the agent\n",
    "AGENT_NAME = \"Seattle Tourist Assistant\"\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"AIPROJECT_CONNECTION_STRING\"],\n",
    ")\n",
    "\n",
    "project_client.agents.enable_auto_function_calls(toolset=toolset)\n",
    "\n",
    "agent = project_client.agents.create_agent(\n",
    "    model=os.environ[\"CHAT_MODEL\"],\n",
    "    name=AGENT_NAME,\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    toolset=toolset,\n",
    ")\n",
    "print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "thread = project_client.agents.create_thread()\n",
    "print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "# Create message to thread\n",
    "MESSAGE = \"Can you fetch me the weather in Seattle?\"\n",
    "\n",
    "message = project_client.agents.create_message(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=MESSAGE,\n",
    ")\n",
    "print(f\"Created message, ID: {message.id}\")\n",
    "\n",
    "run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "print(f\"Run ID: {run.id}\")\n",
    "\n",
    "# display messages\n",
    "for message in project_client.agents.list_messages(thread.id, order=\"asc\").data:\n",
    "    print(f\"Role: {message.role}\")\n",
    "    print(f\"Content: {message.content[0].text.value}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec8cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter for Azure AI agents\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "# Specify the thread and run id\n",
    "thread_id = thread.id\n",
    "run_id = run.id\n",
    "\n",
    "converted_data = converter.convert(thread_id, run_id)\n",
    "print(json.dumps(converted_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "# specify a file path to save agent output (which is evaluation input data)\n",
    "filename = os.path.join(os.getcwd(), \"evaluation_input_data.jsonl\")\n",
    "\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread_id, filename=filename) \n",
    "\n",
    "print(f\"Evaluation data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33731ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator\n",
    "from azure.ai.projects.models import ConnectionType\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"AIPROJECT_CONNECTION_STRING\"],\n",
    ")\n",
    "model_config = project_client.connections.get_default(\n",
    "                                            connection_type=ConnectionType.AZURE_OPEN_AI,\n",
    "                                            include_credentials=True) \\\n",
    "                                         .to_evaluator_model_config(\n",
    "                                            deployment_name=\"gpt4o\",\n",
    "                                            api_version=\"2025-03-01-preview\",\n",
    "                                            include_credentials=True\n",
    "                                          )\n",
    "\n",
    "# select evaluators\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "\n",
    "# batch run API\n",
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "response = evaluate(\n",
    "    data=\"evaluation_input_data.jsonl\",\n",
    "    evaluation_name=\"agent demo - batch run\",\n",
    "    evaluators={\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "    },\n",
    "    # optionally, log your results to your Azure AI Foundry project for rich visualization \n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"AZURE_PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"AZURE_RESOURCE_GROUP_NAME\"],\n",
    "    }\n",
    ")\n",
    "# look at the average scores \n",
    "print(response[\"metrics\"])\n",
    "# use the URL to inspect the results on the UI\n",
    "print(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
