I need your to research a tangible plan to help me scale my model. 
1. Come up with a detailed plan with experiments on how I can improve the model. The plan should document how to tachically experiment and critera to preempt the experiment run. 
2. Research the model architecture and propose changes.

Background: My data is time series dataset and the goal of the model is to train embedding using time step masking. After several experiments I chose the ProgressiveLSTMTransformerReconstructor architecture to had the best performance. I then used optuna to run 50 experiments to select the best params on the baseline dataset. I trained the embeedings using that config on 3 scales of dataset. 
Observations: 
1. As the data scaled the final loss only becomes marginally better. The model seems to underperforming
2. Increased the model params to x3 but the scaled model didnt yied significant results. 

    Optuna best config:
    {"feat_dim":16, "embed_dim":108, "nhead":7, "num_blocks":4, "dim_feedforward":70}

    | Dataset | Start Loss | End Loss |
    | Baseline | 0.2548 | 0.1251 |
    | x1.5 | .2137 | .1207 |
    | x2 | .1884 | .1115 |
    | x3 | .1785 | .1058 |

    Scaled config:
    {"feat_dim":16, "embed_dim":182, "nhead":10, "num_blocks":4, "dim_feedforward":239}

    | Dataset | Start Loss | End Loss |
    | x3 | 0.1815 | .1055 |


Model:
class ProgressiveLSTMTransformerReconstructor(nn.Module):
    def __init__(self,
                 feat_dim=30,
                 embed_dim=128,
                 nhead=4,
                 num_blocks=3,  # Number of LSTM+Transformer blocks
                 dim_feedforward=256,
                 dropout=0.1):
        super().__init__()
        
        # Initial projection
        self.input_proj = nn.Linear(feat_dim, embed_dim)
        
        # Positional embeddings
        self.pos_emb = nn.Parameter(torch.randn(1, 15, embed_dim))
        
        # Progressive blocks of LSTM+Transformer
        self.blocks = nn.ModuleList()
        for _ in range(num_blocks):
            self.blocks.append(
                ProgressiveBlock(
                    embed_dim=embed_dim,
                    nhead=nhead,
                    dim_feedforward=dim_feedforward,
                    dropout=dropout
                )
            )
        
        # Final layer norm
        self.final_norm = nn.LayerNorm(embed_dim)
        
        # Output projection
        self.output_proj = nn.Linear(embed_dim, feat_dim)
        
    def forward(self, x):
        # x: (batch, seq_len=15, feat_dim=30)
        
        # Initial projection
        h = self.input_proj(x) + self.pos_emb
        
        # Process through progressive blocks
        for block in self.blocks:
            h = block(h)
            
        # Final normalization
        h = self.final_norm(h)
        
        # Output projection
        return self.output_proj(h)  # → (batch, seq_len, feat_dim)
    
    def get_embeddings(self, x):
        """Extract intermediate embeddings for contrastive learning"""
        # Initial projection (same as forward method)
        h = self.input_proj(x) + self.pos_emb
        
        # Process through progressive blocks and return final embeddings before output projection
        for block in self.blocks:
            h = block(h)
            
        # Final normalization - return embeddings before output projection
        h = self.final_norm(h)
        return h

class ProgressiveBlock(nn.Module):
    def __init__(self,
                 embed_dim=128,
                 nhead=4,
                 dim_feedforward=256,
                 dropout=0.1):
        super().__init__()
        
        # LSTM layer
        self.lstm = nn.LSTM(
            input_size=embed_dim,
            hidden_size=embed_dim,
            num_layers=1,
            batch_first=True,
            bidirectional=True
        )
        
        # Projection to combine bidirectional outputs
        self.lstm_proj = nn.Linear(embed_dim * 2, embed_dim)
        
        # Layer normalization for LSTM output
        self.lstm_norm = nn.LayerNorm(embed_dim)
        
        # Transformer layer
        self.transformer_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu'
        )
        
    def forward(self, x):
        # x: (batch, seq_len, embed_dim)
        
        # LSTM processing with residual connection
        lstm_out, _ = self.lstm(x)  # → (batch, seq_len, 2*embed_dim)
        lstm_out = self.lstm_proj(lstm_out)  # → (batch, seq_len, embed_dim)
        h = x + lstm_out
        h = self.lstm_norm(h)
        
        # Transformer processing
        h_transformer = h.transpose(0, 1)  # → (seq_len, batch, embed_dim)
        h_transformer = self.transformer_layer(h_transformer)
        h = h_transformer.transpose(0, 1)  # → (batch, seq_len, embed_dim)
        
        return h

class CurriculumMasker:
    def __init__(self, start_prob=0.05, end_prob=0.25, epochs=10):
        """
        Implement curriculum learning by gradually increasing masking probability
        
        Args:
            start_prob: starting masking probability
            end_prob: final masking probability
            epochs: number of epochs to linearly increase probability
        """
        self.start_prob = start_prob
        self.end_prob = end_prob
        self.epochs = epochs
        
    def get_mask_prob(self, current_epoch):
        """Calculate current masking probability based on training progress"""
        if current_epoch >= self.epochs:
            return self.end_prob
        
        # Linear increase
        ratio = current_epoch / self.epochs
        return self.start_prob + ratio * (self.end_prob - self.start_prob)
    
    def apply_masking(self, features, current_epoch):
        """Apply dynamic masking with curriculum difficulty"""
        mask_prob = self.get_mask_prob(current_epoch)

        return dynamic_masking(features, mask_prob=mask_prob)
    
def dynamic_masking(features, mask_prob=0.15):  
    """  
    features: (batch, seq_len=15, feat_dim=30), float  
    mask_prob: probability to mask each time‐step  
    Returns:  
      masked_inputs: same shape as features, with masked slots zeroed  
      labels: the original features (we'll only compute loss on masked slots)  
      mask: a boolean tensor of shape (batch, 15) indicating which time‐steps were masked  
    """  
    b, seq_len, feat_dim = features.shape  
  
    # 1) choose which time‐steps to mask  
    prob = torch.full((b, seq_len), mask_prob, device=features.device)  
    time_mask = torch.bernoulli(prob).bool()           # (batch, 15)  
  
    # 2) expand to cover all 30 features at each masked time  
    mask_exp = time_mask.unsqueeze(-1).expand(b, seq_len, feat_dim)  
  
    # 3) zero‐out those features in the input  
    masked_inputs = features.clone()  
    masked_inputs[mask_exp] = 0.0  
  
    # 4) labels are the original features; we'll ignore unmasked slots later  
    return masked_inputs, features, time_mask  

def focal_reconstruction_loss(y_pred, y_true, masked_positions, gamma=2.0, alpha=0.25):
    """
    Focal loss for reconstruction task
    
    Args:
        y_pred: model predictions (flattened to 1D)
        y_true: ground truth values (flattened to 1D)
        masked_positions: boolean tensor indicating which positions were masked (1D)
        gamma: focusing parameter (higher values increase focus on hard examples)
        alpha: balancing parameter
    """
    # Calculate standard MSE loss element-wise
    mse_loss = (y_pred - y_true) ** 2
    
    # Apply focal weighting
    pt = torch.exp(-mse_loss)  # Convert to probability-like term
    focal_weight = alpha * (1 - pt) ** gamma
    
    # Apply masking and compute mean
    masked_loss = mse_loss * focal_weight * masked_positions
    
    return masked_loss.sum() / (masked_positions.sum() + 1e-8)

 model = ProgressiveLSTMTransformerReconstructor(
            feat_dim=feat_dim, 
            embed_dim=embed_dim, 
            nhead=heads, 
            num_blocks=blocks,
            dim_feedforward=ff, 
            dropout=0.001
        ).to(device)