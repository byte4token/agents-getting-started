{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e932e4c-5d55-461e-a313-3a087d8983b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate using Azure AI Evaluation custom evaluators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd3cfd4",
   "metadata": {},
   "source": [
    "## Objective\n",
    "In this notebook we will demonstrate how to use the target functions with the custom evaluators to evaluate an endpoint.\n",
    "\n",
    "This tutorial provides a step-by-step guide on how to evaluate a function\n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- [azure-ai-evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk)\n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend 20 minutes running this sample. \n",
    "\n",
    "## About this example\n",
    "\n",
    "This example demonstrates evaluating a target function using azure-ai-evaluation\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation\n",
    "%pip install azure-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784be308",
   "metadata": {},
   "source": [
    "### Parameters and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257fd898-7ef2-4d89-872e-da9e426aaf0b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "from azure.ai.evaluation import evaluate\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "azure_openai_api_version = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "azure_openai_deployment = os.environ[\"MODEL_DEPLOYMENT_NAME\"]\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352b517-70b0-4f4f-a3ad-bc99eae67b2e",
   "metadata": {},
   "source": [
    "## Target function\n",
    "We will use a simple `endpoint_callback` to get answers to questions from our model. We will use `evaluate` API to evaluate `endpoint_callback` answers\n",
    "\n",
    "`endpoint_callback` needs following environment variables to be set\n",
    "\n",
    "- AZURE_OPENAI_API_VERSION\n",
    "- AZURE_OPENAI_DEPLOYMENT\n",
    "- AZURE_OPENAI_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbfc3a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to set the environment variables if not already set. If set, you can skip this step.\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = azure_openai_api_version\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = azure_openai_deployment\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_openai_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9bb466-324f-42ce-924a-56e1bc52471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def endpoint_callback(query: str) -> dict:\n",
    "    deployment = os.environ.get(\"AZURE_DEPLOYMENT_NAME\")\n",
    "\n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "    oai_client = AzureOpenAI(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_ENDPOINT\"),\n",
    "        api_version=os.environ.get(\"AZURE_API_VERSION\"),\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "\n",
    "    response_from_oai_chat_completions = oai_client.chat.completions.create(\n",
    "        messages=[{\"content\": query, \"role\": \"user\"}], model=deployment, max_tokens=500\n",
    "    )\n",
    "\n",
    "    response_result = response_from_oai_chat_completions.to_dict()\n",
    "    return {\"query\": query, \"response\": response_result[\"choices\"][0][\"message\"][\"content\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641385d-12d8-4ec2-b477-3b1aeed6e86c",
   "metadata": {},
   "source": [
    "## Data\n",
    "Reading existing dataset which has bunch of questions we can Ask Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b47e777f-3889-49c2-bc53-25488dade7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         query       response\n",
      "0               When was United Stated found ?           1776\n",
      "1               What is the capital of France?          Paris\n",
      "2  Who is the best tennis player of all time ?  Roger Federer\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"data.jsonl\", lines=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44181407",
   "metadata": {},
   "source": [
    "## Running Blocklist Evaluator to understand its input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6f56605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from blocklist import BlocklistEvaluator\n",
    "\n",
    "blocklist_evaluator = BlocklistEvaluator(blocklist=[\"bad\", \"worst\", \"terrible\"])\n",
    "\n",
    "blocklist_evaluator(response=\"New Delhi is Capital of India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b63dd-031d-469d-8232-84affd517f0f",
   "metadata": {},
   "source": [
    "## Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d1dd39-f0a3-4392-bf99-14eecda3e2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-21 10:31:49 -0400   35164 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-10-21 10:31:49 -0400   35164 execution.bulk     INFO     Average execution time for completed lines: 0.0 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"TARGET_20251021_143149_130459\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-10-21 14:31:49.130459+00:00\"\n",
      "Duration: \"0:00:01.015536\"\n",
      "\n",
      "2025-10-21 10:31:50 -0400   38220 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-10-21 10:31:50 -0400   38220 execution.bulk     INFO     Average execution time for completed lines: 0.0 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"blocklist_20251021_143150_155483\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-10-21 14:31:50.155483+00:00\"\n",
      "Duration: \"0:00:01.010840\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"blocklist\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:01.010840\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(\n",
    "    data=\"data.jsonl\",\n",
    "    target=blocklist_evaluator,\n",
    "    evaluators={\n",
    "        \"blocklist\": blocklist_evaluator,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d4569-4e1b-4b44-92ed-9063eccb68ae",
   "metadata": {},
   "source": [
    "View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72fa51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcec6443-14a7-410e-9fc2-1411461dc44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.query</th>\n",
       "      <th>inputs.response</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.score</th>\n",
       "      <th>outputs.blocklist.score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was United Stated found ?</td>\n",
       "      <td>1776</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who is the best tennis player of all time ?</td>\n",
       "      <td>Roger Federer</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  inputs.query inputs.response  \\\n",
       "0               When was United Stated found ?            1776   \n",
       "1               What is the capital of France?           Paris   \n",
       "2  Who is the best tennis player of all time ?   Roger Federer   \n",
       "\n",
       "   inputs.line_number  outputs.score  outputs.blocklist.score  \n",
       "0                   0          False                    False  \n",
       "1                   1          False                    False  \n",
       "2                   2          False                    False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results[\"rows\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef355fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-evals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
