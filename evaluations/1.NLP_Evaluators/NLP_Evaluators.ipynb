{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate with quantitative NLP evaluators\n",
    "\n",
    "## Objective\n",
    "This notebook demonstrates how to use NLP-based evaluators to assess the quality of generated text by comparing it to reference text. By the end of this tutorial, you'll be able to:\n",
    " - Understand different NLP evaluators such as `BleuScoreEvaluator`, `GleuScoreEvaluator`, `MeteorScoreEvaluator`, and `RougeScoreEvaluator`.\n",
    " - Evaluate dataset using these evaluators.\n",
    "\n",
    "## Time\n",
    "You should expect to spend about 10 minutes running this notebook.\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "Install the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "#%pip install azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "azure_ai_project=os.environ.get(\"AIPROJECT_CONNECTION_STRING\"),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NLP Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BleuScoreEvaluator\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) score is commonly used in natural language processing (NLP) and machine\n",
    "translation. It is widely used in text summarization and text generation use cases. It evaluates how closely the\n",
    "generated text matches the reference text. The BLEU score ranges from 0 to 1, with higher scores indicating\n",
    "better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "\n",
    "bleu = BleuScoreEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu_score': 0.22961813530951883, 'bleu_result': 'fail', 'bleu_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = bleu(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GleuScoreEvaluator\n",
    "\n",
    "The GLEU (Google-BLEU) score evaluator measures the similarity between generated and reference texts by\n",
    "evaluating n-gram overlap, considering both precision and recall. This balanced evaluation, designed for\n",
    "sentence-level assessment, makes it ideal for detailed analysis of translation quality. GLEU is well-suited for\n",
    "use cases such as machine translation, text summarization, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "\n",
    "gleu = GleuScoreEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gleu_score': 0.4090909090909091, 'gleu_result': 'fail', 'gleu_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = gleu(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeteorScoreEvaluator\n",
    "\n",
    "The METEOR (Metric for Evaluation of Translation with Explicit Ordering) score grader evaluates generated text by\n",
    "comparing it to reference texts, focusing on precision, recall, and content alignment. It addresses limitations of\n",
    "other metrics like BLEU by considering synonyms, stemming, and paraphrasing. METEOR score considers synonyms and\n",
    "word stems to more accurately capture meaning and language variations. In addition to machine translation and\n",
    "text summarization, paraphrase detection is an optimal use case for the METEOR score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "\n",
    "meteor = MeteorScoreEvaluator(alpha=0.9, beta=3.0, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor_score': 0.9067055393586005, 'meteor_result': 'pass', 'meteor_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = meteor(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RougeScoreEvaluator\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate automatic\n",
    "summarization and machine translation. It measures the overlap between generated text and reference summaries.\n",
    "ROUGE focuses on recall-oriented measures to assess how well the generated text covers the reference text. Text\n",
    "summarization and document comparison are among optimal use cases for ROUGE, particularly in scenarios where text\n",
    "coherence and relevance are critical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "\n",
    "rouge = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge_precision': 1.0, 'rouge_recall': 1.0, 'rouge_f1_score': 1.0, 'rouge_precision_result': 'pass', 'rouge_recall_result': 'pass', 'rouge_f1_score_result': 'pass', 'rouge_precision_threshold': 0.5, 'rouge_recall_threshold': 0.5, 'rouge_f1_score_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = rouge(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a Dataset using Math Evaluators\n",
    "\n",
    "The code below uses the Evaluate API with BLEU, GLEU, METEOR, and ROUGE evaluators to evaluate the results on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-04 09:26:40 -0500   28476 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-11-04 09:26:40 -0500   28476 execution.bulk     INFO     Average execution time for completed lines: 0.0 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-11-04 09:26:40 -0500    2764 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-11-04 09:26:40 -0500    2764 execution.bulk     INFO     Average execution time for completed lines: 0.11 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"rouge_20251104_142640_650291\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-11-04 14:26:40.650291+00:00\"\n",
      "Duration: \"0:00:01.010139\"\n",
      "\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"bleu_20251104_142640_634124\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-11-04 14:26:40.634124+00:00\"\n",
      "Duration: \"0:00:01.102229\"\n",
      "\n",
      "2025-11-04 09:26:41 -0500    2932 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-11-04 09:26:41 -0500    2932 execution.bulk     INFO     Average execution time for completed lines: 0.43 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"gleu_20251104_142640_642399\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-11-04 14:26:40.642399+00:00\"\n",
      "Duration: \"0:00:01.501144\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-04 09:26:43 -0500    2676 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-11-04 09:26:43 -0500    2676 execution.bulk     INFO     Average execution time for completed lines: 1.04 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"meteor_20251104_142640_646984\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-11-04 14:26:40.646984+00:00\"\n",
      "Duration: \"0:00:03.128780\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"bleu\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:01.102229\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"gleu\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:01.501144\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"meteor\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:03.128780\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"rouge\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:01.010139\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    evaluation_name=\"basic_evals\",\n",
    "    data=\"data.jsonl\",\n",
    "    evaluators={\n",
    "        \"bleu\": bleu,\n",
    "        \"gleu\": gleu,\n",
    "        \"meteor\": meteor,\n",
    "        \"rouge\": rouge,\n",
    "    },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project=os.environ.get(\"AIPROJECT_CONNECTION_STRING\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rows': [{'inputs.response': 'The cat sits on the mat.',\n",
       "   'inputs.ground_truth': 'A cat is sitting on the mat.',\n",
       "   'outputs.bleu.bleu_score': 0.3768499164492419,\n",
       "   'outputs.bleu.bleu_result': 'fail',\n",
       "   'outputs.bleu.bleu_threshold': 0.5,\n",
       "   'outputs.gleu.gleu_score': 0.4230769230769231,\n",
       "   'outputs.gleu.gleu_result': 'fail',\n",
       "   'outputs.gleu.gleu_threshold': 0.5,\n",
       "   'outputs.meteor.meteor_score': 0.7454289732770746,\n",
       "   'outputs.meteor.meteor_result': 'pass',\n",
       "   'outputs.meteor.meteor_threshold': 0.5,\n",
       "   'outputs.rouge.rouge_precision': 0.6666666666666666,\n",
       "   'outputs.rouge.rouge_recall': 0.5714285714285714,\n",
       "   'outputs.rouge.rouge_f1_score': 0.6153846153846153,\n",
       "   'outputs.rouge.rouge_precision_result': 'pass',\n",
       "   'outputs.rouge.rouge_recall_result': 'pass',\n",
       "   'outputs.rouge.rouge_f1_score_result': 'pass',\n",
       "   'outputs.rouge.rouge_precision_threshold': 0.5,\n",
       "   'outputs.rouge.rouge_recall_threshold': 0.5,\n",
       "   'outputs.rouge.rouge_f1_score_threshold': 0.5,\n",
       "   'line_number': 0},\n",
       "  {'inputs.response': 'She enjoys reading books.',\n",
       "   'inputs.ground_truth': 'She loves to read books.',\n",
       "   'outputs.bleu.bleu_score': 0.1098261401671543,\n",
       "   'outputs.bleu.bleu_result': 'fail',\n",
       "   'outputs.bleu.bleu_threshold': 0.5,\n",
       "   'outputs.gleu.gleu_score': 0.2222222222222222,\n",
       "   'outputs.gleu.gleu_result': 'fail',\n",
       "   'outputs.gleu.gleu_threshold': 0.5,\n",
       "   'outputs.meteor.meteor_score': 0.8203389830508474,\n",
       "   'outputs.meteor.meteor_result': 'pass',\n",
       "   'outputs.meteor.meteor_threshold': 0.5,\n",
       "   'outputs.rouge.rouge_precision': 0.5,\n",
       "   'outputs.rouge.rouge_recall': 0.4,\n",
       "   'outputs.rouge.rouge_f1_score': 0.4444444444444445,\n",
       "   'outputs.rouge.rouge_precision_result': 'pass',\n",
       "   'outputs.rouge.rouge_recall_result': 'fail',\n",
       "   'outputs.rouge.rouge_f1_score_result': 'fail',\n",
       "   'outputs.rouge.rouge_precision_threshold': 0.5,\n",
       "   'outputs.rouge.rouge_recall_threshold': 0.5,\n",
       "   'outputs.rouge.rouge_f1_score_threshold': 0.5,\n",
       "   'line_number': 1},\n",
       "  {'inputs.response': 'He quickly ran to the store.',\n",
       "   'inputs.ground_truth': 'He ran to the store in a hurry.',\n",
       "   'outputs.bleu.bleu_score': 0.3419177649965185,\n",
       "   'outputs.bleu.bleu_result': 'fail',\n",
       "   'outputs.bleu.bleu_threshold': 0.5,\n",
       "   'outputs.gleu.gleu_score': 0.4,\n",
       "   'outputs.gleu.gleu_result': 'fail',\n",
       "   'outputs.gleu.gleu_threshold': 0.5,\n",
       "   'outputs.meteor.meteor_score': 0.6392045454545455,\n",
       "   'outputs.meteor.meteor_result': 'pass',\n",
       "   'outputs.meteor.meteor_threshold': 0.5,\n",
       "   'outputs.rouge.rouge_precision': 0.8333333333333334,\n",
       "   'outputs.rouge.rouge_recall': 0.625,\n",
       "   'outputs.rouge.rouge_f1_score': 0.7142857142857143,\n",
       "   'outputs.rouge.rouge_precision_result': 'pass',\n",
       "   'outputs.rouge.rouge_recall_result': 'pass',\n",
       "   'outputs.rouge.rouge_f1_score_result': 'pass',\n",
       "   'outputs.rouge.rouge_precision_threshold': 0.5,\n",
       "   'outputs.rouge.rouge_recall_threshold': 0.5,\n",
       "   'outputs.rouge.rouge_f1_score_threshold': 0.5,\n",
       "   'line_number': 2}],\n",
       " 'metrics': {'bleu.bleu_score': 0.27619794053763824,\n",
       "  'bleu.bleu_threshold': 0.5,\n",
       "  'gleu.gleu_score': 0.3484330484330484,\n",
       "  'gleu.gleu_threshold': 0.5,\n",
       "  'meteor.meteor_score': 0.7349908339274891,\n",
       "  'meteor.meteor_threshold': 0.5,\n",
       "  'rouge.rouge_precision': 0.6666666666666666,\n",
       "  'rouge.rouge_recall': 0.5321428571428571,\n",
       "  'rouge.rouge_f1_score': 0.5913715913715913,\n",
       "  'rouge.rouge_precision_threshold': 0.5,\n",
       "  'rouge.rouge_recall_threshold': 0.5,\n",
       "  'rouge.rouge_f1_score_threshold': 0.5,\n",
       "  'bleu.binary_aggregate': 0.0,\n",
       "  'gleu.binary_aggregate': 0.0,\n",
       "  'meteor.binary_aggregate': 1.0,\n",
       "  'rouge.binary_aggregate': 0.67},\n",
       " 'studio_url': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Checking result display capabilities ===\")\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "# Check if result exists and what type it is\n",
    "print(\"Result exists:\", 'result' in locals())\n",
    "if 'result' in locals():\n",
    "    print(\"Type of result:\", type(result))\n",
    "    print(\"Result has __repr__:\", hasattr(result, '__repr__'))\n",
    "    print(\"Result has __str__:\", hasattr(result, '__str__'))\n",
    "    \n",
    "    # Check if it's a custom object with special display behavior\n",
    "    print(\"Result class name:\", result.__class__.__name__)\n",
    "    print(\"Result module:\", result.__class__.__module__)\n",
    "    \n",
    "    # Try different ways to display\n",
    "    print(\"\\n=== Direct print ===\")\n",
    "    print(result)\n",
    "    \n",
    "    print(\"\\n=== Using pprint ===\")\n",
    "    pprint(result)\n",
    "    \n",
    "    print(\"\\n=== Using repr ===\")\n",
    "    print(repr(result))\n",
    "else:\n",
    "    print(\"No result to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing basic output:\")\n",
    "print(\"Hello world\")\n",
    "pprint({\"test\": \"data\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-evals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
